name: Benchmark Matrix (Cross-OS)

on:
  workflow_dispatch:
    inputs:
      mode:
        description: "Benchmark mode"
        required: true
        default: synthetic
        type: choice
        options:
          - synthetic
          - real

permissions:
  contents: read

jobs:
  benchmark-matrix:
    name: Benchmark (${{ matrix.label }})
    runs-on: ${{ matrix.os }}
    timeout-minutes: 40
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            label: linux
            baseline: benchmark/baseline.linux.json
            python_bin: python3
            abs_floor_ms: 5.0
          - os: macos-latest
            label: macos
            baseline: benchmark/baseline.macos.json
            python_bin: python3
            abs_floor_ms: 7.0
          - os: windows-latest
            label: windows
            baseline: benchmark/baseline.windows.json
            python_bin: python
            abs_floor_ms: 8.0

    env:
      CARGO_TERM_COLOR: always
      CRABCLAW_BENCH_MODE: ${{ github.event.inputs.mode }}
      CRABCLAW_BENCH_BASELINE: ${{ github.event.inputs.mode == 'real' && 'benchmark/baseline.real.json' || matrix.baseline }}
      PYTHON_BIN: ${{ matrix.python_bin }}
      CRABCLAW_BENCH_STRICT: '1'
      CRABCLAW_BENCH_ABS_FLOOR_MS: ${{ matrix.abs_floor_ms }}
      CRABCLAW_BENCH_REAL_PROVIDER_URL: ${{ secrets.CRABCLAW_BENCH_REAL_PROVIDER_URL }}
      CRABCLAW_BENCH_REAL_PROVIDER_API_KEY: ${{ secrets.CRABCLAW_BENCH_REAL_PROVIDER_API_KEY }}
      CRABCLAW_BENCH_REAL_PROVIDER_MODEL: ${{ secrets.CRABCLAW_BENCH_REAL_PROVIDER_MODEL }}
      CRABCLAW_BENCH_REAL_CHANNEL_WEBHOOK_URL: ${{ secrets.CRABCLAW_BENCH_REAL_CHANNEL_WEBHOOK_URL }}
      CRABCLAW_BENCH_REAL_TOOL_COMMAND: ${{ secrets.CRABCLAW_BENCH_REAL_TOOL_COMMAND }}

    steps:
      - uses: actions/checkout@v4
      - uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: 1.92
      - uses: Swatinem/rust-cache@v2

      - name: Validate real-mode prerequisites
        if: github.event.inputs.mode == 'real'
        shell: bash
        run: |
          set -euo pipefail
          if [ -z "${CRABCLAW_BENCH_REAL_PROVIDER_URL:-}" ] || [ -z "${CRABCLAW_BENCH_REAL_PROVIDER_API_KEY:-}" ]; then
            echo "Real mode requested but provider secrets are missing" >&2
            exit 1
          fi

      - name: Run benchmark gate
        id: bench
        shell: bash
        run: bash scripts/benchmark_ci.sh

      - name: Write matrix status file
        if: always()
        shell: bash
        run: |
          mkdir -p benchmark/results
          cat > benchmark/results/_status.env <<EOF
          label=${{ matrix.label }}
          os=${{ matrix.os }}
          mode=${{ github.event.inputs.mode }}
          result=${{ steps.bench.outcome }}
          EOF

      - name: Append summary to job summary
        if: always()
        shell: bash
        run: |
          if [ -f benchmark/results/summary.md ]; then
            cat benchmark/results/summary.md >> "$GITHUB_STEP_SUMMARY"
          else
            echo "## ⚠️ Benchmark summary unavailable" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.label }}-${{ github.event.inputs.mode }}
          path: benchmark/results/

  benchmark-matrix-summary:
    name: Benchmark Matrix Summary
    if: always()
    needs: [benchmark-matrix]
    runs-on: ubuntu-latest
    steps:
      - name: Download benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
          pattern: benchmark-results-*-${{ github.event.inputs.mode }}
          merge-multiple: false

      - name: Build consolidated summary
        shell: bash
        run: |
          python3 - <<'PY'
          import json
          from pathlib import Path

          root = Path('artifacts')
          rows = []
          detail = []
          failed = False

          if not root.exists():
            Path('matrix-summary.md').write_text('## ❌ Benchmark Matrix Summary\n\nNo artifacts found.\n')
            raise SystemExit(1)

          for d in sorted([p for p in root.iterdir() if p.is_dir()]):
            status_file = d / '_status.env'
            summary_file = d / 'summary.md'
            latest_file = d / 'latest.full.json'

            label = d.name
            result = 'unknown'
            os_name = 'unknown'
            if status_file.exists():
              env = {}
              for line in status_file.read_text().splitlines():
                if '=' in line:
                  k, v = line.split('=', 1)
                  env[k.strip()] = v.strip()
              label = env.get('label', label)
              result = env.get('result', 'unknown')
              os_name = env.get('os', 'unknown')

            status_icon = '✅' if result == 'success' else ('❌' if result in ('failure','cancelled','timed_out') else '⚠️')
            rows.append((label, os_name, result, status_icon))
            if result != 'success':
              failed = True

            if latest_file.exists():
              try:
                data = json.loads(latest_file.read_text())
                m = data.get('metrics', {})
                detail.append((
                  label,
                  m.get('cold_start.median_ms'),
                  m.get('cold_start.p90_ms'),
                  m.get('ttft.median_ms'),
                  m.get('ttft.p90_ms'),
                  m.get('cost.per_task_usd')
                ))
              except Exception:
                detail.append((label, None, None, None, None, None))
            else:
              detail.append((label, None, None, None, None, None))

          lines = ['## Benchmark Matrix Summary', '', '| Label | OS Runner | Result | Status |', '|---|---|---|---|']
          for label, os_name, result, icon in rows:
            lines.append(f'| `{label}` | `{os_name}` | `{result}` | {icon} |')

          lines.extend([
            '',
            '### Key Metrics (gate-aligned from latest.full.json)',
            '',
            '| Label | cold_start.median_ms | cold_start.p90_ms | ttft.median_ms | ttft.p90_ms | cost.per_task_usd |',
            '|---|---:|---:|---:|---:|---:|'
          ])
          for label, cold_median, cold_p90, ttft_median, ttft_p90, cost in detail:
            def fmt(v):
              return '-' if v is None else f"{v:.4f}"
            lines.append(
              f'| `{label}` | {fmt(cold_median)} | {fmt(cold_p90)} | {fmt(ttft_median)} | {fmt(ttft_p90)} | {fmt(cost)} |'
            )

          Path('matrix-summary.md').write_text('\n'.join(lines) + '\n')
          print('\n'.join(lines))
          if failed:
            raise SystemExit(1)
          PY

      - name: Append matrix summary to job summary
        if: always()
        run: cat matrix-summary.md >> "$GITHUB_STEP_SUMMARY"

      - name: Upload matrix summary artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-matrix-summary-${{ github.event.inputs.mode }}
          path: matrix-summary.md
